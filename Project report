AI Podcast Transcription & Diarization System

Project Goal: To build an automated pipeline that takes raw podcast audio and converts it into a professional, speaker-labeled script with summaries.

1. Data Collection (download_kaggle_subset.py)

What's in it: A script that connects to Kaggle and downloads a specific amount of data (e.g., 2GB).

What we used: kaggle (Python API).

Why: Real podcast datasets are massive (100GB+). We needed a way to download just a small, manageable "slice" for testing without filling up the hard drive.

How it works:

It authenticates using your API key.

It lists files in the dataset.

It downloads audio files one by one until the 2GB limit is reached, then stops automatically.

2. Audio Preprocessing (preprocess_audio.py)

What's in it: A cleaning machine that standardizes all audio files.

What we used: librosa (for audio analysis) and soundfile (for saving).

Why: AI models are picky. They perform best when audio is consistent. Raw MP3s can be stereo, noisy, or have different sample rates, which slows down or confuses the AI.

How it works:

Format: Converts compressed MP3s to uncompressed WAV.

Resampling: Changes sample rate to 16,000 Hz (16kHz). Viva Tip: This is the native rate OpenAI Whisper was trained on.

Mono: Mixes Stereo (Left/Right) into Mono (One track) because speech models don't need directional sound.

Skipping: Checks if a file is already processed to save time.

3. Transcription & Summarization (transcribe_podcasts.py)

What's in it: The "Ears" and "Brain" of the project. It listens to audio and writes text.

What we used:

OpenAI Whisper: For Speech-to-Text (Transcription).

Hugging Face Transformers (DistilBART): For abstractive summarization.

Why:

Whisper: It is currently the state-of-the-art open-source model for accuracy.

JSON: We save data as JSON (not just text) because we need the exact timestamps (start/end time) of every sentence for the next step.

How it works:

Transcription: It feeds the clean WAV file into Whisper. Whisper outputs text segments with timestamps.

Interactive Mode: After processing, it asks if you want a summary.

Smart Summary: It takes the full text, feeds it into DistilBART, and generates a concise 200-250 word summary of the podcast.

4. Speaker Diarization (diarize_podcasts.py)

What's in it: The "Who said what" engine. This turns a block of text into a script.

What we used: pyannote.audio.

Why: Whisper knows what was said, but it is bad at knowing who said it. Pyannote is specialized for "Speaker Diarization" (identifying unique voices).

How it works:

Parallel Processing: It runs Pyannote on the audio to get a list of "Who was speaking when" (e.g., Speaker 1: 00:00-00:05).

Merging: It loads the JSON timestamps from Whisper.

Alignment: It matches the timestamps. If Whisper says "Hello" at 00:02, and Pyannote says "Speaker A" was active at 00:02, the script labels that line as "Speaker A".

Key Technologies (Cheat Sheet)

Whisper (OpenAI): Converts Audio → Text.

Pyannote (Hugging Face): Identifies Voices (Diarization).

Librosa: Handles audio loading and mathematics.

FFmpeg: The engine under the hood that reads/writes audio formats.

Technical Stack & Implementation Details (For Viva/Interview)

If asked "How exactly did you build this?", use these technical details:

1. Audio Processing Stack

Library: librosa (The industry standard for audio analysis in Python).

Code Implementation:

y, sr = librosa.load(audio_path, sr=16000, mono=True)


Why 16,000 Hz? This is the specific sampling rate the Whisper model was trained on. Giving it 44.1kHz audio forces it to downsample internally, wasting CPU cycles. We do it proactively.

Why Mono? Speech recognition models analyze a single waveform. Stereo channels contain redundant data.

2. Transcription Model

Library: openai-whisper

Model Architecture: Transformer-based Encoder-Decoder.

Specific Model Used: base model (~74M parameters).

Note: We chose 'base' as a trade-off between speed and accuracy. The 'large' model is more accurate but requires 10GB+ VRAM.

Code Implementation:

model = whisper.load_model("base")
result = model.transcribe(audio_file, fp16=False)


Timestamps: We extract result['segments'] to get start/end times for every sentence, which is critical for mapping speakers later.

3. Summarization Model

Library: transformers (by Hugging Face).

Model Architecture: BART (Bidirectional and Auto-Regressive Transformers).

Specific Model Used: sshleifer/distilbart-cnn-12-6.

Why this model? It is a "distilled" (smaller/faster) version of the massive BART model, fine-tuned specifically on the CNN/DailyMail dataset for news summarization. It is perfect for condensing factual content.

Code Implementation:

summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
summary = summarizer(text_chunk, max_length=300, min_length=150)


4. Speaker Diarization (Segmentation)

Library: pyannote.audio

Specific Model Used: pyannote/speaker-diarization-3.1.

How it works (Under the hood): It uses an end-to-end neural network that performs three tasks:

Voice Activity Detection (VAD): "Is someone speaking?"

Speaker Embedding: Creates a vector (mathematical fingerprint) for the voice.

Clustering: Groups similar vectors together to assign labels (Speaker 0, Speaker 1).

Code Implementation:

pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
diarization = pipeline(audio_file)


##

Phase 1: The Brain (podcast_backend.py)
This single file is the engine. It performs a sequence of 6 distinct tasks whenever a file is uploaded.

1. Preprocessing (Cleaning the Data)
The Problem: Raw audio files (MP3s) vary wildly. Some are stereo, some are mono, some are high quality (44kHz), some are low. AI models hate inconsistency.

The Solution: We use Librosa to force every audio file into a standard format: 16,000 Hz Mono WAV.

Why? The OpenAI Whisper model was trained specifically on 16kHz audio. If you feed it 44kHz, it has to downsample it internally anyway (wasting time). By doing it ourselves, we make the pipeline faster and more robust.

2. Transcription (The Foundation)
The Tech: OpenAI Whisper (Base model).

How it works: Whisper uses a Transformer architecture (Encoder-Decoder). It treats the audio like an image (a spectrogram), analyzes the patterns, and predicts the text.

Crucial Detail: We don't just ask for text; we ask for Timestamps. We need to know that "Hello world" was said from 00:00 to 00:02. Without this, we couldn't build the sentiment graph or topic segments later.

3. Summarization (The Executive Summary)
The Tech: DistilBART (via Hugging Face Transformers).

Logic: This is an Abstractive summarizer. Unlike old methods that just highlighted important sentences ("Extractive"), this AI reads the text, "understands" it, and writes a new summary from scratch, similar to how a human would summarize a book.

4. Sentiment Analysis (The Emotional Curve)
The Tech: VADER (NLTK).

Why VADER? Modern Deep Learning models (like BERT) are great but slow. VADER is a "Lexicon-based" tool—it has a massive dictionary of words with emotional ratings (e.g., "Good" = +1.9, "Disaster" = -3.1).

The Workflow: We feed it every single sentence. It calculates a score between -1 (Negative) and +1 (Positive). We map these scores to the timestamps to draw the line graph you see on the dashboard.

5. Topic Segmentation (The "Chapter" Maker)
This is the most mathematically complex part. We want to find where the conversation shifts topics.

Step A (Vectorization - TF-IDF): Computers don't understand words; they understand numbers. We use TF-IDF to turn every sentence into a "Vector" (a list of numbers representing the importance of words).

Step B (Similarity - Cosine Similarity): We measure the "angle" between two sentence vectors.

If Sentence A talks about Music and Sentence B talks about Songs, the angle is small (High Similarity).

If Sentence B talks about Songs and Sentence C talks about Politics, the angle is wide (Low Similarity).

Step C (The Cut): The script scans through the text using a "Sliding Window." When the similarity score drops below our threshold (0.5), the system assumes a topic change occurred and cuts the segment there.

6. Keyword Extraction (The Tagger)
The Tech: TF-IDF.

Logic: TF-IDF looks for words that appear frequently in this specific podcast but rarely in general English. This effectively filters out boring words like "the", "and", "is", leaving you with unique, content-rich keywords like "Negotiation", "Climate", or "Algorithm".

Phase 2: The Face (app.py)
This file handles the User Experience (UX). It uses Streamlit.

It is "Stateless": Streamlit runs the entire script from top to bottom every time you click a button.

It does NOT do Math: Notice that app.py mostly just reads the JSON/Text files created by the backend. This "decoupling" is a good software engineering practice. It means if the UI crashes, your data is safe in the files.

Visualization: It uses Plotly to take the Sentiment JSON data (Timestamp vs. Score) and draw the interactive green/red chart.
